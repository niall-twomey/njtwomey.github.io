<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Niall Twomey</title>
  <meta name="description" content="The personal website of Niall Twomey cataloguing publications, projects, blog and code snippets.">

  <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicon.ico">
  <link rel="icon" href="http://localhost:4000/assets/img/favicon.ico">
  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title"><a href='/'><strong>Niall</strong> Twomey</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="http://localhost:4000/">about</a>

        <!-- Blog -->
        <a class="page-link" href="http://localhost:4000/blog/">blog</a>

        <!-- Pages --><a class="page-link" href="http://localhost:4000/projects/">projects</a><a class="page-link" href="http://localhost:4000/publications/">publications</a><!-- CV link -->
        <!-- <a class="page-link" href="http://localhost:4000/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="header-bar">
  <h2>Notes, snippets and articles!</h2>
  <h3></h3>
</div>

<ul class="post-list">
  
    
    <li>
      <h2><a class="post-title" href="/blog/2019/bringing-focus-back-to-your-inbox/">Bringing focus back to your inbox</a></h2>
      <p class="post-meta">
          08 November 2019
           &middot; Niall Twomey
           &middot; <a href="/tags/2019">2019</a> &middot; <a href="/tags/snippet">snippet</a> &middot; <a href="/tags/email">email</a> &middot; <a href="/tags/javascript">javascript</a> &middot; <a href="/tags/python">python</a>
      </p>
      
        <br>
       
      <p> I realised recently that my primary email account is barely fulfilling its purpose anymore. The main cause&colon; over-subscription has devalued the utility of the basic service. In other words, since the majority of the emails I received have basically zero value, I give much less attention to those that are actually valuable. I decided to take action and brutally cull and unsubscribe from all but the most important or valuable services. The main problem with this decision is that I am fairly lazy so wanted to do as little manual labor as possible. Since I'm also rather forgetful and will surely need to do this again in the future I'm really hoping that this post will help the future me! </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2019/universal-background-model/">Classical Universal Background Model for Unsupervised Adaptation</a></h2>
      <p class="post-meta">
          01 November 2019
          
           &middot; <a href="/tags/2019">2019</a>
      </p>
      
        <br>
       
      <p>  </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2019/taking-the-fourier-transform-of-your-cat/">Taking the Fourier transform of your cat</a></h2>
      <p class="post-meta">
          26 June 2019
          
           &middot; <a href="/tags/2019">2019</a> &middot; <a href="/tags/snippet">snippet</a> &middot; <a href="/tags/fun">fun</a> &middot; <a href="/tags/python">python</a>
      </p>
      
        <div class="profile col one left">
            <div class='img_row'>
  <a href="http://localhost:4000/assets/2019/taking-the-fourier-transform-of-your-cat/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2019/taking-the-fourier-transform-of-your-cat/thumb.png"/>
  </a>
</div>
        </div>
       
      <p> There are a few YouTube channels that I watch fairly regularly that have recently been making some truly beautiful visualisations of the Fourier transform. The visualisations really were beautiful (I'll link to them in the text) not only aesthetically but also in how they intuitively show what the Fourier transform works and what it achieves. I was inspired to try to replicate the visualisation procedure and the code inside this post is a python implementation of the method. The approach itself in truth isn't terribly complicated, but I find the outcome hypnotic! </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2019/neural-odes-with-stochastic-vector-field-mixtures/">Neural ODEs with stochastic vector field mixtures</a></h2>
      <p class="post-meta">
          23 May 2019
          
           &middot; <a href="/tags/2019">2019</a> &middot; <a href="/tags/research">research</a> &middot; <a href="/tags/ode">ode</a> &middot; <a href="/tags/preprint">preprint</a>
      </p>
      
        <div class="profile col one right">
            <div class='img_row'>
  <a href="http://localhost:4000/assets/2019/neural-odes-with-stochastic-vector-field-mixtures/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2019/neural-odes-with-stochastic-vector-field-mixtures/thumb.png"/>
  </a>
</div>
        </div>
       
      <p> It was recently shown that neural ordinary differential equation models cannot solve fundamental and seemingly straightforward tasks even with high-capacity vector field representations. This paper introduces two other fundamental tasks to the set that baseline methods cannot solve, and proposes mixtures of stochastic vector fields as a model class that is capable of solving these essential problems. Dynamic vector field selection is of critical importance for our model, and our approach is to propagate component uncertainty over the integration interval with a technique based on forward filtering. We also formalise several loss functions that encourage desirable properties on the trajectory paths, and of particular interest are those that directly encourage fewer expected function evaluations. Experimentally, we demonstrate that our model class is capable of capturing the natural dynamics of human behaviour; a notoriously volatile application area. Baseline approaches cannot adequately model this problem. </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2019/storm/">Ordinal regression as structured classification</a></h2>
      <p class="post-meta">
          20 May 2019
          
           &middot; <a href="/tags/2019">2019</a> &middot; <a href="/tags/research">research</a> &middot; <a href="/tags/crf">crf</a> &middot; <a href="/tags/ordinal">ordinal</a>
      </p>
      
        <div class="profile col one left">
            <div class='img_row'>
  <a href="http://localhost:4000/assets/2019/storm/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2019/storm/thumb.png"/>
  </a>
</div>
        </div>
       
      <p> This paper extends the class of ordinal regression models with a structured interpretation of the problem by applying a novel treatment of encoded labels. The net effect of this is to transform the underlying problem from an ordinal regression task to a (structured) classification task which we solve with conditional random fields, thereby achieving a coherent and probabilistic model in which all model parameters are jointly learnt. Importantly, we show that although we have cast ordinal regression to classification, our method still fall within the class of decomposition methods in the ordinal regression ontology. This is an important link since our experience is that many applications of machine learning to healthcare ignores completely the important nature of the label ordering, and hence these approaches should considered naive in this ontology. We also show that our model is flexible both in how it adapts to data manifolds and in terms of the operations that are available for practitioner to execute. Our empirical evaluation demonstrates that the proposed approach overwhelmingly produces superior and often statistically significant results over baseline approaches on forty popular ordinal regression models, and demonstrate that the proposed model significantly out-performs baselines on synthetic and real datasets. Our implementation, together with scripts to reproduce the results of this work, will be available on a public GitHub repository. </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2019/hgpad-journal/">An application of hierarchical Gaussian processes to the detection of anomalies in star light curves</a></h2>
      <p class="post-meta">
          04 February 2019
          
           &middot; <a href="/tags/2019">2019</a> &middot; <a href="/tags/research">research</a> &middot; <a href="/tags/gp">gp</a> &middot; <a href="/tags/astro">astro</a>
      </p>
      
        <div class="profile col one right">
            <div class='img_row'>
  <a href="http://localhost:4000/assets/2019/hgpad-journal/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2019/hgpad-journal/thumb.png"/>
  </a>
</div>
        </div>
       
      <p> This study is concerned with astronomical time-series called light-curves that represent the brightness of celestial objects over a period of time. We consider the task of finding anomalous light-curves of periodic variable stars. We employ a Hierarchical Gaussian Process to create a general and stable model of time-series for anomaly detection, and apply this approach to the light-curve problem. Hierarchical Gaussian Processes require only a few additional parameters compared to conventional Gaussian Processes and incur negligible additional computational complexity. Moreover, since the additional parameters are objectively optimised in a principled probabilistic framework one does not need to resort to grid searches for parameter selection. Experimentally, we demonstrate that our approach outperforms several baselines on both synthetic and light-curve data. Of particular interest is that the proposed method generalises very well from small subsets of the data, achieving near perfect precision of outlier detection even with as few as seven instances. </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2018/efficient/">Efficient Approximate Representations of Computationally Expensive Features</a></h2>
      <p class="post-meta">
          01 April 2018
          
           &middot; <a href="/tags/2018">2018</a>
      </p>
      
        <div class="profile col one left">
            <div class='img_row'>
  <a href="http://localhost:4000/assets/2018/efficient/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2018/efficient/thumb.png"/>
  </a>
</div>
        </div>
       
      <p> High computational complexity is often a barrier to achieving desired representations in resource-constrained settings. This paper introduces a simple and computationally cheap method of approximating complex features. We do so by carefully constraining the architecture of Neural Networks (NNs) and regress from raw data to the intended feature representation. Our analysis focuses on spectral features, and demonstrates how low-capacity networks can capture the end-to-end dynamics of cascaded composite functions. Not only do approximating NNs simplify the analysis pipeline, but our approach produces feature representations up to 20 times more quickly. Excellent feature fidelity is achieved in our experimental analysis with feature approximations, but we also report nearly indistinguishable predictive performance when comparing between exact and approximate representations </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2017/unsupervised/">Unsupervised learning of sensor topologies for improving activity recognition in smart environments</a></h2>
      <p class="post-meta">
          19 April 2017
          
           &middot; <a href="/tags/2017">2017</a>
      </p>
      
        <div class="profile col one right">
            <div class='img_row'>
  <a href="http://localhost:4000/assets/2017/unsupervised/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2017/unsupervised/thumb.png"/>
  </a>
</div>
        </div>
       
      <p> There has been significant recent interest in sensing systems and ‘smart environments’, with a number of longitudinal studies in this area. Typically the goal of these studies is to develop methods to predict, at any one moment of time, the activity or activities that the resident(s) of the home are engaged in, which may in turn be used for determining normal or abnormal patterns of behaviour (e.g. in a health-care setting). Classification algorithms, such as Conditional Random Field (CRFs), typically consider sensor activations as features but these are often treated as if they were independent, which in general they are not. Our hypothesis is that learning patterns based on combinations of sensors will be more powerful than single sensors alone. </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2016/bdl/">BDL. NET: Bayesian dictionary learning in Infer. NET</a></h2>
      <p class="post-meta">
          13 September 2016
          
           &middot; <a href="/tags/2016">2016</a>
      </p>
      
        <div class="profile col one left">
            <div class='img_row'>
  <a href="http://localhost:4000/assets/2016/bdl/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2016/bdl/thumb.png"/>
  </a>
</div>
        </div>
       
      <p> We introduce and analyse a flexible and efficient implementation of Bayesian dictionary learning for sparse coding. By placing Gaussian-inverse-Gamma hierarchical priors on the coefficients, the model can automatically determine the required sparsity level for good reconstructions, whilst also automatically learning the noise level in the data, obviating the need for heuristic methods for choosing sparsity levels. This model can be solved efficiently using Variational Message Passing (VMP), which we have implemented in the Infer.NET framework for probabilistic programming and inference. We analyse the properties of the model via empirical validation on several accelerometer datasets. We provide source code to replicate all of the experiments in this paper. </p>
    </li>
  
    
    <li>
      <h2><a class="post-title" href="/blog/2016/need/">On the need for structure modelling in sequence prediction</a></h2>
      <p class="post-meta">
          01 September 2016
          
           &middot; <a href="/tags/2016">2016</a>
      </p>
      
        <div class="profile col one right">
            <div class='img_row'>
  <a href="http://localhost:4000/assets/2016/need/thumb.png">
    <img class="col three" src="http://localhost:4000/assets/2016/need/thumb.png"/>
  </a>
</div>
        </div>
       
      <p> There is no uniform approach in the literature for modelling sequential correlations in sequence classification problems. It is easy to find examples of unstructured models (e.g. logistic regression) where correlations are not taken into account at all, but there are also many examples where the correlations are explicitly incorporated into a—potentially computationally expensive—structured classification model (e.g. conditional random fields). In this paper we lay theoretical and empirical foundations for clarifying the types of problem which necessitate direct modelling of correlations in sequences, and the types of problem where unstructured models that capture sequential aspects solely through features are sufficient. The theoretical work in this paper shows that the rate of decay of auto-correlations within a sequence is related to the excess classification risk that is incurred by ignoring the structural aspect of the data. </p>
    </li>
  
</ul>

<div class="pagination clearfix mb1 mt4">
  <div class="left"><span class="pagination-item disabled">Newer</span></div>
  <div class="right"><a class="pagination-item" href="/blog/page2/">Older</a></div>
  <div class="pagination-meta">Page1of2</div>
</div>
<div class="social">
  
    <div class="col three caption">
      Contact me through the links below.
    </div>
  
  <span class="contacticon center">
    <a href="mailto:%74%77%6F%6D%65%79%6E%6A %61%74 %67%6D%61%69%6C %64%6F%74 %63%6F%6D"><i class="fas fa-envelope" style="font-size:0.5em;"></i></a>
    <a href="https://orcid.org/0000-0002-3225-2654" style="font-size:0.5em;" target="_blank" title="ORCID"><i class="ai ai-orcid"></i></a>
    <a href="https://scholar.google.com/citations?user=bRN8Y34AAAAJ" style="font-size:0.5em;" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
    <a href="https://github.com/njtwomey" style="font-size:0.5em;" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
    <a href="https://www.linkedin.com/in/nialltwomey" style="font-size:0.5em;" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
    <a href="https://twitter.com/twomeynj" style="font-size:0.5em;" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
    
    
    
  </span>
</div>



      </div>
    </div>

    <footer>
  <div class="wrapper col three caption">
    &copy; Copyright 2019 Niall Twomey
    
    
  </div>
</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="http://localhost:4000/assets/js/common.js"></script><!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="http://localhost:4000/assets/js/katex.js"></script><!-- Include custom icon fonts -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-151975494-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-151975494-1');
</script>



  </body>

</html>
